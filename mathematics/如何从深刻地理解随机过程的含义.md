作者：许铁-巡洋舰科技
链接：https://www.zhihu.com/question/26694486/answer/349872296
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

## 第一部分：为什么要研究随机过程？
人类认识世界的历史，就是一认识和描绘各种运动的历史，从宏观的天体运动到分子的运动，到人心理的运动-我们通称为变化，就是一个东西随时间的改变。

人们最成功的描绘运动的模型是牛顿的天体运动，确定性是牛顿体系最大的特征。给定位置和速度，运动轨迹即确定。但是20世纪后的科学却失去了牛顿美丽的确定性光环。因为当人们试图描绘一些真实世界，充满复杂而未知因素的运动时候，人们发现不确定的因素(通常称之为噪音)对事物的变化至关重要，而牛顿理论描述的只是完美场景。因此我们应用概率来定量分析事物变化的不确定性，与之相应的产生的一个全新的研究运动的方法-随机过程, 对不确定性下的运动进行精细的数学描述。

在大数据时代, 我们周边充满了各种各样的数据，这些数据是真实世界的映射，最基本的特点就是含有巨量的噪音， 而随机过程就是从这些噪音里提取信息的武器。

<font color=blue size=3>其实我们生活中也处处充满“噪音”。比如说我们每天发邮件，经常有一些人时回时不回。那些不回的人到底是忘了还是真的不想回，我们却不知道。一个书呆子统计学家会告诉你，你无法从一次的行为评判他，而要看他一贯的表现。</font>

第一个随机过程方法的伟大胜利是爱因斯坦的布朗运动。一些小花粉在水里，受到水分子不停碰撞，而呈现随机的运动(花粉颗粒由于很小比较容易受到水分子热扰动的影响)。研究这些花粉的微小运动似乎有点天然呆，我们却从中找到了分子世界重要的信息。而花粉那无序与多变的轨道，也为我们提供了随机运动的范式(随机游走)。
<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/)
<p align="center"><font color=gray size=2>计算机生成的十个粒子的布朗运动轨迹</font></p>
<br>

如果给随机过程打个比方，它就像是一个充满交叉小径的花园。你站在现在的点上，看未来的变化，未来有千万种变化的方式， 每一种可能又不断分叉变化出其它可能。

## 第二部分： 描述随机过程的武器

随机过程怎么研究？几样神器是不可缺少的。 

### 1. 概率空间

面对不可确定的未来，我们可以从两个维度来描述，一个是有哪些可能的结果，一个是每种可能的大小， 前者定义一个事件空间(态空间)， 后者定义一个数-概率。 那么这些信息从哪里来呢？ 我们如何知道要发生什么？ 又如何知道多多大可能发生？ -- 历史。 

概率论的出发点其实是: 日光之下并无新事。我们假设一切客观世界的存在都是周期性的，在这个基础上我们模糊掉时间，对未来的预测来源于对已发生事实的观测和分析，而这种观测和分析的结果就是概率。

一件事发生可能性的大小，就是一件事在历史中发生的频率。当然很多情况下概率也可以通过已知理论用演绎法推得，但是最根本的，还是由经验确定的概率。 概率，我们中学数学都学过它是一个事件出现的频率，但它的含义其实很深很深。因为一个事件出现的频率来自于历史，而概率却用于对未来的预测，因此，<font color=red size=5>概率包含的一个基本假设就是未来和过去的一致性</font>-你要用概率，你所研究的对象要有可重复性。

这其实假设了概率所研究的事件具有的某种周期稳定性，一旦这些一个过程是一个随时间无序变化的过程，概率几乎就不能应用。所以这里只能说概率是一种近似，他对于研究那些比较简单的物理过程，如投掷硬币，才完全有效。所以，所谓概率空间，只能是一种近似，他是人类现有知识的总和，我们用它描述已知的未知，但是却从来无法描述未知的未知-被我们称作黑天鹅的事件，因为真正的未来，永远无法只有已知的可能性(感兴趣的请参看本人旧文-高斯与天鹅)。在大多数时候，我们还是日光之下并无新事，因此，概率论的威力依然不可小觑。

有关概率空间的思维，可以立刻灭掉一些看似烧脑实际脑残的题目：假设你在进行一个游戏节目。现给三扇门供你选择：一扇门后面是一辆轿车，另两扇门后面什么都没有。你的目的当然是得到轿车，但你却并不能看到门后的真实情况。主持人先让你作第一次选择。在你选择了一扇门后，知道其余两扇门后面是什么的主持人，打开了另一扇门给你看，而且，当然，那里什么都没有。现在主持人告诉你，你还有一次选择的机会。那么，请你考虑一下，你是坚持第一次的选择不变，还是改变第一次的选择，更有可能得到轿车？

回答这个问题的关键即事件空间，在主持人打开门之前，事件空间即车的位置有三种可能，你有 1/3 的可能拿到车。当主持人选择打开门的时候，它实际上帮你做了一个选择，那就是告诉你某个车库没有车，这时候事件空间发生了变化，因为你的已知变了。以前的事件空间中，三个车库有车的概率都是 1/3。现在的情况是打开的车库有车的概率变为0， 因此你选择的车库没车的情况下车的位置已经变成确定的了，概率为2/3。而原来你车库有车的选项却不受到这一事件的影响(依然1/3概率)，所以你当然要选择换车库。这个例子第一个说明的道理是概率是主观的，来自于你头脑中的信息。主持人的举动增加了你对两个车库的信息，而车是不变的，所以你要根据新的信息调整概率空间。

此实例是好的思维方法的力量的典范，如果你没有这个事件空间的角度，恐怕要做无数的试验了。    

条件概率：现实生活中的一般都以条件概率的形式出现，即给定一定的已知条件，我们会得到什么样的概率。对这一大类问题可以引出整个贝叶斯分析理论，将在后续篇章中介绍。

### 2. 随机变量
你投掷筛子，可能得到的结果有6个，每种结果有 1/6 的可能。你把态空间的种种可能性都用数字表达出来，就是随机变量。 这个东西包含所有输出的可能性以及相应的概率，这些可能性(态空间)和概率的对应关系我们称之为分布函数。如果态空间是连续的，我们就得到连续的分布函数形式。

<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/)
<p align="center"><font color=gray size=2>一个二维高斯分布</font></p>
<br>

* **分布函数**：随机变量已经包含了两个随机过程研究的核心武器：态空间和分布函数。分布函数是提取随机过程内有用信息的第一手段。分布函数-是在大量数据中提取信息的入口。
<br>

* **随机变量的实现**：随机变量可以看做一个实验，你在实验之前，结果是不确定的，你所有的是一团可能性。 当你做完实验，却得到一个唯一的结果，只是预先不可知。
<br>

* **期望**： 对一个随机变量，已知其分布函数，可以定义一个期望。这个东西由每个结果的取值和它的可能性共同决定，表达未来结果的加权平均值。 
<br>

&emsp;&ensp;实际中我们可以用实验的方法确定这个数字，就是所谓蒙特卡罗方法，不停的投筛子然后做个统计，你所得到的结果的平均就是期望。(平均值和期望的区别就是第一个来自已有的数据的平均，第二是对根据已有的平均对未来的预测)。

&emsp;&ensp;关于期望包含着一种投资世界里的基本思维方式，就是对收益的幅值和风险(概率)一起考虑。经常有一些时候一些出现机会极少而收益特别大的可能性决定了期望，如果你的心脏足够强大，就应该充分考虑这些高风险高收益的可能。  
<br>
相关性：  对于两个随机变量，你可以定义一个相关性covariance，描述一个随机变量随另一个而变化的趋势。这个函数特别有用，它是现实生活中我们说两个事物相关性的精确表达。
<br>


$$\sigma(x, y) = \mathrm{E}[(x-\mathrm{E}[x])(y-\mathrm{E}[y])]$$

理解这个算式特别简单，E[x] 是随机变量 x 的期望值(期望值是该变量输出值的平均数), 这个公式就是计算 x 和 y 波动乘积的期望值，当两个变量是此消彼长，则为负，共生共荣则为正，若两个过程不相关，则为 0.
方差：  上述关系当 x=y 我们得到方差，方差就是自己和自己的关联函数，当随机变量比较接近正态分布时候它可以描绘波动性的大小。对于 N 个随机变量，任意两个随机变量可得到一个 covariance，而这样一组 covariance 构成大名鼎鼎的 covariance matrix.
<br>
$$\Sigma = \begin{pmatrix} \mathrm{E}[(\mathrm{X}_1-\mu_1)(\mathrm{X}_1-\mu_1)] & \mathrm{E}[(\mathrm{X}_1-\mu_1)(\mathrm{X}_2-\mu_2)] & \cdots & \mathrm{E}[(\mathrm{X}_1-\mu_1)(\mathrm{X}_n-\mu_n)] \\ \mathrm{E}[(\mathrm{X}_2-\mu_2)(\mathrm{X}_1-\mu_1)] & \mathrm{E}[(\mathrm{X}_2-\mu_2)(\mathrm{X}_2-\mu_2)] & \cdots & \mathrm{E}[(\mathrm{X}_2-\mu_2)(\mathrm{X}_n-\mu_n)] \\ \vdots & \vdots & \vdots & \vdots\\ \mathrm{E}[(\mathrm{X}_n-\mu_n)(\mathrm{X}_1-\mu_1)] & \mathrm{E}[(\mathrm{X}_n-\mu_n)(\mathrm{X}_2-\mu_2)]  & \cdots & \mathrm{E}[(\mathrm{X}_n-\mu_n)(\mathrm{X}_n-\mu_n)] \end{pmatrix}$$

* 测量分布函数的武器-蒙特卡罗方法：
搞定一个分布函数，笨办法也是最有用的方法就是蒙特卡罗方法。一般骰子情况下，骰子有6个面， 每个面出现的概率有1/6，但是万一骰子被做过手脚呢？ 所以最好的方法还是所谓蒙特卡罗抽样，不停的玩，知道你认为你可以稳定得到每次可能性出现的频率。所谓笨办法确是最常用的，尤其是随着高速计算机的普及。一些重大的工程，涉及太多复杂不好确定因素时候，我们就让计算机模拟，设计一系列的蒙特卡罗抽样来求得一些结果。
<br>
* 抽样：
在计算机里研究牵扯随机变量的过程最基本的方法就是抽样，抽样就是已知分布函数取得一个随机的结果的过程。我们要在计算机里模拟一个随机过程都是通过抽样来实现的。抽样的成功与否决定这些计算机模拟(simulation)能在多少程度逼近真实。计算机的抽样都是基于最简单的随机数生成器产生的，产生概率均等的均与分布(Uniform distribution)。但是这些“随机数”实际是早已设定好的，因此更准备的被称作“伪随机数”。而对于更加复杂的分布函数的抽样， 则有如层出不穷的算法解决它，比如大名鼎鼎的 Markov Chain Monte Carlo (MCMC)方法，将在之后的章节介绍。


## 第三部分： 什么是随机过程
&emsp;&ensp;确定性过程研究一个量随时间确定的变化，而随机过程描述的是一个量随时间可能的变化，在这个过程里，每一个时刻变化的方向都是不确定的，或者说随机过程就是由一系列随机变量组成，每一个时刻系统的状态都由一个随机变量表述，而整个过程则构成态空间的一个轨迹(随机过程的实现)。
&emsp;&ensp;一个随机过程最终实现，会得到一组随时间变化的数值(态空间里的轨迹)，实践中我们都是从数据结果中推测一个随机过程的性质的。
&emsp;&ensp;刚说过概率是建立在可重复性上，是一个理想模型，而建立在此上的随机过程就更是一个理想化的模型，它暗含的是历史可无限重复，然后你把他们收集在一起看一看。我在一开头的说的充满分叉小径的花园是一种比喻，但说的也是你需要站在平时时空 (每一个时空包含一种历史的可能性) 的角度来看一个随机过程的全貌。  
&emsp;&ensp;我们立刻发现这是一个超级复杂的问题，因为一个随机过程具有无限多可能性。试想象一个最简单的随机过程，这个过程由N步组成，每一步都有两个选择(0，1)，那么可能的路径就有2的N次方个，这个随机过程就要由2^N-1个概率来描述(概率只和为一减掉一个维度)，用数学物理的语言就是极高维度的问题。
* <font color=blue size=3>离散的时间序列是清晰表述随机过程的入门方式，虽然更一般的表述是时间是连续的</font>

因此，能否研究一个随机过程的关键就是减少问题的维度-这也是物理的核心思想。

**马尔科夫过程(Markov Processes)**
&emsp;&ensp;马尔科夫过程，是随机过程中的精华部分，其地位犹如牛顿定律在力学的地位。 对于最一般的随机过程，是无限复杂的，幸好，在我们日常生活中，很多随机过程符合或近似更简单的模型。其中目前一种最有效的框架成为马尔科夫过程. 所谓马尔科夫过程，即随机过程的每一步的结果最多只与上一步有关，而与其它无关。 好比你不停撒筛子，你每一次的结果不会影响未来的成绩。 

**马尔可夫链(Markov chain)**
&emsp;&ensp;makov 过程用数学语言表述就是马尔科夫链，就像一台雄雄驶过的火车，前一个车厢(上一步)拉着后一个(下一步)，向前运行。如果一个过程是markov过程，这个过程就得到了神简化，你只需要知道第n步是如何与第n-1步相关的，一般由一组条件概率表述，就可以求得整个过程。一个巨大的随机过程，其内核仅仅是这样一组条件概率，而知道了这组条件概率，就可以衍生整个过程。
<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/)
<p align="left"><font color=gray size=2>一个典型的 markov 过程， 每一个的结果只与上一步相关，我们只需要一组条件概率(箭头)来描述，每个条件概率告你如果态空间中的某一个事件发生，那么从这一点出发， 下一个事件发生的概率。</font></p>
<br>
&emsp;&ensp;我们不妨多想一下，如果第n步和第n-1步的关系不是随机的，而是确定的，那我们得到了什么？我们联想到牛顿力学，牛顿力学也是此刻的状态决定下一刻的变化，其本质也是链式法则，通过此刻与此刻最邻近的未来的关系，衍生出整个宇宙的过去和未来， 其灵魂同样是降维。或者说markov就是随机过程里的牛顿法则。

&emsp;&ensp;Markov 是不是真的是一个历史无关的过程？ No！ 虽然第 N+1 步只与第 N 步有关，但是第N步又包含第 N-1 步，所以通过链式法则，历史的信息还是可以传递到现在的。
<br>
经典表述：

$$Pr=(X_{n+1}=x|X_1=x_1,X_2=x_2,\cdots,X_n=x_n)=Pr(X_{n+1}=x|X_n=x_n)$$

&emsp;&ensp;马尔科夫链的核心条件概率表达式就是这台火车链接不同车厢的链条。 如果这个条件概率关系不随时间变化，我们就得到经典的稳态马尔科夫链。它有一个良好的性质，就是当这个过程启动一段时间就会进入统计稳态，稳态的分布函数与历史路径无关。

一个简单的例子： 关于生育偏好是否影响男女比例的问题

我们知道过去的人喜欢生男孩，往往生女孩子就不停生，直到生到一个男生为止，因此就造成很多一大堆姐姐只有一个弟弟的家庭。我接触过的一些特别聪明的人都会认为这样的行为会影响男女比例。大部分人觉得会造成女孩比例多，少数人认为会增加男孩比例。实际呢？  

**一言以蔽之： 不变.**     为什么？  生育问题是典型的稳态马尔科夫过程，下一次生育不受上一次生育的影响。 根据马氏过程的特性，你知道历史无需考虑历史路径， 最终的平衡概率只取决于每一步的概率。所以无论你怎么玩，不论是你拼命想生男孩还是女孩，都无法影响人口比例。

但是有一招却是有影响的，就是打胎。 为什么？ 答案依然很简单，你改变了每一步的概率。 

这就是马尔科夫过程的威力和魅力，可惜人生却不是马尔科夫过程， 因为每一步都高度依赖于过去 n 步，因此人生是高度历史路径依赖的。

**当一个随机过程的变化只取决于当下的变化而非历史的时候，我们得到一个马尔科夫链条。它的优良性质使得巨大的计算瞬时简化。**

**进一步降维：**
markov链的思维用一组前一步和后一步的条件概率关系衍生整个过程，具有巨大的简化威力。对于更加特殊的问题，维度还可以继续降低，问题得意更彻底的简化。 例如：

**稳态过程-stationary process:**
如果说markov过程每一步与前一步的关系是与时间无关的，或符合

$$Pr=(X_{n+1}=x|X_n=y)=Pr(X_n=x|X_{n-1}=y)$$

这个过程就是稳态的，这个时候我们只需要这样一个关系就描述整个过程。

&emsp;&ensp;在这个极度简化的模型下，**markov process** 可归结为一个在态空间里的跃迁轨迹。下图的随机变量是横轴(a，b，c，d四个态)，时间是纵轴。系统从此刻的态跃迁到下一刻的态都是随机的，而且跃迁的概率由一个数字决定，这个数字不由轨迹的历史决定，因而 markov。从此刻任一状态到达下一刻任意状态包含 4x4 个概率，因此可以写作一个 4x4 的跃迁矩阵。跃迁矩阵 $P_{ij}$ 涵盖了过程的全部信息。

<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/)
<p align="left"><font color=gray size=2>跃迁矩阵 </font></p>
<br>

**稳态过程顾名稳态， 是因为在一段时间后系统会进入一个平衡状态，或者说系统的分布函数不随时间变化.** 如同上文提到的人口中男女比例问题，男女比例在各个国家都在 1：1 左右， 就是因为生成它的过程是一个稳态过程。

**稳态过程含有两个个重要的特征量： 平均值和自相关函数(Auto-correlation)**，稳态(stationary)的含义正是在平均值附近扰动，在这个情况下随机性换以另外一个名词-fluctuation(扰动)。 而在非稳态下，扰动和平均值的概念变得模糊，失去意义。

&emsp;&ensp;平均值自然重要，但扰动却往往包含着平均值所没有的信息。 首先我们计算方差，来看扰动的剧烈程度，但是这远远不够。

&emsp;&ensp;Auro-correlation和之前描述的相关性具有内在的联系，事实上它描述的就是此时的扰动和彼时的扰动的相关性。

$$R_\tau=\dfrac{\mathrm{E}[(\mathrm{X}_t-\mu)(\mathrm{X}_{t+\tau}-\mu)]}{\sigma^2}$$
&emsp;&ensp;这个量可以理解为你手里有一个信号，首先你减去平均值，这样信号就在0附近扰动。 你把这个信号平行移动一个时间差， 然后把它和原来的信号乘起来，如果说信号本身代表的过程在时间上胡乱跳跃无迹可寻， 那么这个量就很接近0，因为正和负的部分无序的乘起来，正负互相抵消，你的期望就是0。反之，如果你的信号内包含内在的构造(pattern)，就会得到不为0的值。

**因此，日常生活中你手里具有的往往是数据，你什么都不知道的时候，计算这个量就是起点，这个东西在帮你寻找无序中的结构(pattern)，它将告诉我们系统噪音的性质。**

&emsp;&ensp;比如我们经常说的白色噪声(white noise)的定义就是自关联性为0， 因为它要的是绝对的无序， 毫无记忆，毫无结构。这种信号就是最基本的噪声形态。

&emsp;&ensp;而如果我们发现一个随时间差变化很慢的自相关函数，往往显示系统具有记忆的特性，因而产生了更复杂的结构， 或者系统临近相变。

&emsp;&ensp;自相关性的计算告诉我们的是， 你不要只看表面的无序有序，因为人眼喜欢在无序中寻找有序，而一个有力的计算就可以告诉你比你的眼睛更准确的信息。
<br>
### master equation

&emsp;&ensp;刚才描述离散的markov过程，**如果一个过程是连续的，不再分为第一步第二步第三步， 我们就可以用微分方程描述一个马尔科夫过程。 这就是master equation - 所谓大师方程。** 这是物理，化学，经济学，得到一些给力结果经常用到的微分方程。

&emsp;&ensp;master equation直接关注的是随机过程的全貌。刚才所说的跃迁轨迹是一次实验的结果，而Master Equation 描述的却是无数实验者同时入场，进行马尔科夫过程，你会看到一个新的图像。系统每一个时刻的状态不再是态空间一个具体的点，而是一大团点(一大丛实验者)，它们慢慢的在态空间里运动，我们可以统计站在不同的状态上的实验者个数，因而得到的是一个概率分布，正是之前说的分布函数的概念。 物理经常用概率云，概率波一类的词描述这种情境。 其实都是在说我们不再用一个数字描述世界，比如速度，位置，而是这个值的分布函数。变化的不再是某个特定的值而是它的分布函数。

&emsp;&ensp;态空间的分布函数，又可称作场。由此，场的物理学可以徐徐入场。 

&emsp;&ensp;之前说的马尔科夫过程的关键-联系此刻与下一刻的条件概率，在这里以跃迁矩阵 A 表示。

&emsp;&ensp;刚才讲到牛顿力学和马尔科夫过程有着内在的联系，**Master equation 就是随机过程里的牛顿第二定律。**这个方程对于解释很多物理化学里的随机过程有神一般的效力。他就是**概率场的动力学方程**。

$$\dfrac{d_{\overrightarrow{P}}}{d_t}=A(t)\overrightarrow{P}$$


&emsp;&ensp;A 是跃迁矩阵，而向量 P 即概率场，就是经过时间 t，系统状态的分布函数。该方程描述的是概率会怎么变。

&emsp;&ensp;由此我们看到用Maser方程研究问题的好处，转不确定为确定。当你站在纵览所有可能性的制高点，把所有可能性看做高维空间的“概率场”。 不确定性的随机游走变成了概率分布函数(概率场)的确定性演化。- 这也是为什么场物理在近代物理后成为主导，所研究对象多为随机过程。

* <font color=blue size=3>量子力学大名鼎鼎的薛定谔方程，其实说的也是这回事，我们无法同时确定电子的位置和动量，因为我们转而求其概率分布函数， 得到一个类似 Master equation 的微分方程，只不过数学形式更复杂，但思维都是转而研究概率的动力学。 这个方程却干掉了一个物理史上的超级难题， 如果在考虑微观世界的不可确定下预测它们的运动。</font>

$$\mathrm{H}(t)|\psi(t)\rangle={i\hbar}\dfrac{\partial}{\partial_t}|\psi(t)\rangle$$
&emsp;&ensp;$|\psi(t)\rangle$ 希尔伯特空间中 $\psi(t)$ 为元素的向量(Bra–ket notation)
&emsp;&ensp;H 表征波函数总能量的哈密顿算符
&emsp;&ensp;$\hbar$ 约化普朗克常数
&emsp;&ensp;$\psi$ 是物理系统的波函数
&emsp;&ensp;i 虚数单位
&emsp;&ensp;${\partial}/{\partial_t}$ 对于时间 t 的偏微分
<p align="center"><font color=gray size=2>薛定谔方程的形式和Master Equation 十分类似。只不过这里的用波函数而不用概率场，但两者其实由一个简单关系一一对应 </font></p>

* <font color=blue size=3>随机事件的重要方程，无论是物理里的郎之万方程，还是金融期权定价的方程，都直接与 Master Equation 相关。</font>

* <font color=blue size=3>稳态解：master equation 指导系统演化，如果A（t）不含时间， 就得到刚才说的稳态过程，系统会演化成一个稳定状态，即分布函数不再随时间变化。A*P=0 我们通常称为平衡态。</font>

* <font color=blue size=3>熵：对应一个平衡态，我们可以定义系统的熵，或者说系统的不确定性，可能性的选项越多，可能性越均匀，这个值就越大。</font>
<br>
<br>

### 经典的 markov 例子 Branching process：

&emsp;&ensp;分叉过程 ，一个祖先繁衍的后代， 会出现多少个家庭， 每个家庭人口是怎么分布的？

&emsp;&ensp;所有家族的演化，生物种群的繁殖，都可以用这个模型研究。一个个体可以繁殖出的子嗣数量是一个随机变量，经过n代之后将形成一个由大小迥异的家族组成的群体。

&emsp;&ensp;如果对应为一个随机过程：-每一代的人口数就是就是随机变量，我们要研究的就是与这个随机变量对应的分布函数。

&emsp;&ensp;这个过程具有的典型性质是迭代： 如果上一代的人口数 Gn，下一代就是 Gn+1=G(Gn)，给定第 n 代的家族人口分布，那么下一代的家族人口分布只与上代有关. 所以这个是典型的 Markov process.

&emsp;&ensp;这个问题可以推出一些有趣的问题，比如人口中各大姓氏的比例。 一般情况下，各大姓氏的比例在各个种群中符合相同的统计规律(幂律)，就是 Branching Process 的结果。

<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/)
<br>

<br>
### 经典的 markov 例子 Poisson Process：

&emsp;&ensp;高中党皆知的随机过程，比如一个小旅店里一晚上到来的客人数量随时间的变化，或者光子枪喷出的光子数, 一个帖子两分钟内的访问次数，都是再经典不过的例子了。

&emsp;&ensp;泊松分布由二项分布演化而来。二项分布十分好理解，给你n次机会抛硬币，硬币正面向上概率为 p，那么n此抛出有 k 次朝上的概率有多少？ 这是一个经典的二项分布。当这里的概率 p 趋于 0，而 n 趋于无穷，我们就得到一个泊松分布。泊松分布多用于连续时间上的问题， 如果概率在连续的时间上是均匀不变的(任意时候发生的概率为 P)，我们就有一个泊松过程。这也极好理解，只要你把时间切割成小段。 比如打开一个帖子的两分钟访问者的概率分布问题，你把两分钟分成 120 秒， 每秒上有访问者进入的概率是确定的，那么这无非就是投 120 次硬币多少次向上的问题， 由于微小时间尺度上一件事情发生的概率通常很小，因此，泊松分布通常成立。

$$\mathrm{P}(X=x)=\dfrac{\lambda^xe^{-x}}{x!}$$

<p align="center"><font color=gray size=2>泊泊松分布的形式，x及事件发生的次数</font></p>

<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/)
<p align="center"><font color=gray size=2>泊松分布一般的形状，三条曲线代表了平均值不同的三个泊松分布 </font></p>
<br>

&emsp;&ensp;泊松过程，恐怕是最简单的随机过程，也是所有随机过程的参考系-好比物理的惯性定律。我们研究一个随机过程时候，第一个做的就是与泊松做比较。

&emsp;&ensp;为什么泊松是一切随机过程的参考系？因为泊松是一个此时的变化和彼时毫无联系的过程，或者说此刻和下一刻是完全独立的，markov说的是与此时只允许与上一个时刻有联系，而泊松就更近一步，把这种联系也取消掉。

&emsp;&ensp;如果我们假定每件事件的发生都与其它时刻事件的发生无关，我们就可以试图用泊松分布表述它。比如一个商店前台顾客的光临，一般情况下，每一个顾客的到来都与前一个顾客无关，因此一段时间内前台顾客的数量符合泊松分布。

&emsp;&ensp;反过来，判断一个随机过程的前后事件是否独立，也可以通过它是否符合泊松分布判别，如果你得到的统计分析偏离了泊松，通过是前后事件相关联的标志。 事实上生活中的事情都偏离泊松，而是具有强大的关联性。 比如你一周内收到的邮件，通过在周一早上爆发而来，而在周末减少到零。你在一段时间会不停叫桃花运，而后一段十分冷清等。 这些都告诉你要找找背后的原因。
<br>
### 经典的 markov 例子 Wiener Process：
&emsp;&ensp;Wiener Process， 其原型就是大名鼎鼎的布朗运动。这恐怕是在自然科学以及经济金融里用的最广泛的随机过程。也是随机过程的灵魂基础。

&emsp;&ensp;关于Wiener Process， 最有趣的比喻是随机游走的醉汉。醉汉在一条直线上移动，往左或往右的概率相等。醉汉走出去的距离与时间的关系，就是Winner Process。

<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/)
<p align="center"><font color=gray size=2>Wiener Process, 上上下下的随机游走表现的美丽轨迹，也是众多股市爱好者经常看到的形状</font></p>
<br>

&emsp;&ensp;Wiener Process 所依赖的假设特别简单： 醉汉走出的每一步的距离和上一步无关(依然在说马氏性)，而这一步走出的长度是由一个确定的高斯分布产生的随机数。 如果这个高斯分布的期望为0，那么这个过程就是一个纯粹的随机游走，反之则是一个但有漂移(drift)的随机游走。

&emsp;&ensp;股票和期货等的价格规律，最基本的假设就是随机游走，在此之上可以得到一些简单的定价模型。 但是事实上， 这种规律只在短期内成立，一旦金融危机爆发， 模型就终止了。 而金融危机，依然是过程内部的长程关联的表现。 因为市场的交易毕竟不是随机的，股市的涨落引起人们心情和预期的变化，从而以正反馈的形式给股市，所谓涨则疯买，低则疯卖，这种关联性打破了随机游走的梦
<br>
### 信息在哪里？

&emsp;&ensp;说了这么半天随机过程，起核心的应用却还没有谈，如何在一个随机性的变化过程中，提取信息?

&emsp;&ensp;首先，变化过程从来都是一些数据记录的，dirty data，肮脏混乱的数据，你要把这些 data 输入到一个电脑程序中，用我说的前面那套东西搞它。随机过程的重要性就在这个数据里提取信息的过程。

&emsp;&ensp;怎么搞，分两步，正问题和反问题：

#### 反问题-数据出发：

1. 数据可视化。因为数据杂乱无章，你几乎看不到任何信息，你要做的第一个工作就是让杂乱的数据平均化，平均，才容易观察趋势。那么何为平均化？-低通滤镜，去掉不必要的高频信息。 这里的关键是时间窗口，时间窗口就是你用来作平局的数据尺度，时间窗口内的数据你都用其平均数代替。 时间窗口的选择学问很大，一般越大容易看整体变化的趋势，越小则可以精细统计细节信息。 而最好的做法是在平均时候变化时间窗口，观察数据链是如何随时间窗口大小变化的。

2. 计算分布函数。选择恰当的变量计算分布函数。随机过程的关键信息就在分布函数里。每一种特定的随机过程，都有特定分布函数对应。因此，从分布函数识别随机过程，就是反向判断的关键。

3. 寻找相关性： 信息就是那些多次重复中随机过程中不变的数据信息。所以提取信息首先要足够数据。然后计算不同次试验数据之间的相关性，相关性大小是数据信息含量的直接指示。

4. 统计学习： 基于贝叶斯分析的统计学习将在后续篇章叙述。他是目前从数据里提取信息的大势所趋(state of art)。

#### 正问题-模型出发：

&emsp;&ensp;要判断由数据推测出来的随机过程对不对，就反过来进行模型模拟， 模型将产生与试验类似的数据，这个时候我们就可以看我们猜测的模型正确了多少。 比如刚才说的泊松过程就是最简单的模型。 往往我们可以先假定一个过程是泊松过程，然后就可以推得一组分布函数，把推得的分布函数和实际从数据中观测的分布函数比较，我们就可以知道我们和这个最简单的模型的偏差。 模型也是一个循序渐进不断修正的过程， 这点依然和时下流行的统计学习有关。





