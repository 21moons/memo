作者：许铁-巡洋舰科技
链接：https://www.zhihu.com/question/26694486/answer/349872296
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

## 第一部分：为什么要研究随机过程？
人类认识世界的历史，就是一认识和描绘各种运动的历史，从宏观的天体运动到分子的运动，到人心理的运动-我们通称为变化，就是一个东西随时间的改变。
人们最成功的描绘运动的模型是牛顿的天体运动，确定性是牛顿体系最大的特征。给定位置和速度，运动轨迹即确定。但是20世纪后的科学却失去了牛顿美丽的确定性光环。因为当人们试图描绘一些真实世界，充满复杂而未知因素的运动时候，人们发现不确定的因素(通常称之为噪音)对事物的变化至关重要，而牛顿理论描述的只是完美场景。因此我们应用概率来定量分析事物变化的不确定性，与之相应的产生的一个全新的研究运动的方法-随机过程, 对不确定性下的运动进行精细的数学描述。
在大数据时代, 我们周边充满了各种各样的数据，这些数据是真实世界的映射，最基本的特点就是含有巨量的噪音， 而随机过程就是从这些噪音里提取信息的武器。
其实我们生活中也处处充满“噪音”。比如说我们每天发邮件，经常有一些人时回时不回。那些不回的人到底是忘了还是真的不想回，我们却不知道。一个书呆子统计学家会告诉你，你无法从一次的行为评判他，而要看他一贯的表现。第一个随机过程方法的伟大胜利是爱因斯坦的布朗运动。一些小花粉在水里，受到水分子不停碰撞，而呈现随机的运动(花粉颗粒由于很小比较容易受到水分子热扰动的影响)。研究这些花粉的微小运动似乎有点天然呆，我们却从中找到了分子世界重要的信息。而花粉那无序与多变的轨道，也为我们提供了随机运动的范式(随机游走)。
<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/hadoop/hadoop_modules.png)
<p align="center"><font color=gray size=2>计算机生成的十个粒子的布朗运动轨迹</font></p>
<br>

如果给随机过程打个比方，它就像是一个充满交叉小径的花园。你站在现在的点上，看未来的变化，未来有千万种变化的方式， 每一种可能又不断分叉变化出其它可能。

## 第二部分： 描述随机过程的武器

随机过程怎么研究？几样神器是不可缺少的。 

### 1. 概率空间

面对不可确定的未来，我们可以从两个维度来描述，一个是有哪些可能的结果，一个是每种可能的大小， 前者定义一个事件空间(态空间)， 后者定义一个数-概率。 那么这些信息从哪里来呢？ 我们如何知道要发生什么？ 又如何知道多多大可能发生？ -- 历史。 
概率论的出发点其实是: 日光之下并无新事。我们假设一切客观世界的存在都是周期性的，在这个基础上我们模糊掉时间，对未来的预测来源于对已发生事实的观测和分析，而这种观测和分析的结果就是概率。
一件事发生可能性的大小，就是一件事在历史中发生的频率。当然很多情况下概率也可以通过已知理论用演绎法推得，但是最根本的，还是由经验确定的概率。 概率，我们中学数学都学过它是一个事件出现的频率，但它的含义其实很深很深。因为一个事件出现的频率来自于历史，而概率却用于对未来的预测，因此，<font color=red size=5>概率包含的一个基本假设就是未来和过去的一致性</font>-你要用概率，你所研究的对象要有可重复性。
这其实假设了概率所研究的事件具有的某种周期稳定性，一旦这些一个过程是一个随时间无序变化的过程，概率几乎就不能应用。所以这里只能说概率是一种近似，他对于研究那些比较简单的物理过程，如投掷硬币，才完全有效。所以，所谓概率空间，只能是一种近似，他是人类现有知识的总和，我们用它描述已知的未知，但是却从来无法描述未知的未知-被我们称作黑天鹅的事件，因为真正的未来，永远无法只有已知的可能性(感兴趣的请参看本人旧文-高斯与天鹅)。在大多数时候，我们还是日光之下并无新事，因此，概率论的威力依然不可小觑。
有关概率空间的思维，可以立刻灭掉一些看似烧脑实际脑残的题目：假设你在进行一个游戏节目。现给三扇门供你选择：一扇门后面是一辆轿车，另两扇门后面什么都没有。你的目的当然是得到轿车，但你却并不能看到门后的真实情况。主持人先让你作第一次选择。在你选择了一扇门后，知道其余两扇门后面是什么的主持人，打开了另一扇门给你看，而且，当然，那里什么都没有。现在主持人告诉你，你还有一次选择的机会。那么，请你考虑一下，你是坚持第一次的选择不变，还是改变第一次的选择，更有可能得到轿车？
回答这个问题的关键即事件空间，在主持人打开门之前，事件空间即车的位置有三种可能，你有 1/3 的可能拿到车。当主持人选择打开门的时候，它实际上帮你做了一个选择，那就是告诉你某个车库没有车，这时候事件空间发生了变化，因为你的已知变了。以前的事件空间中，三个车库有车的概率都是 1/3。现在的情况是打开的车库有车的概率变为0， 因此你选择的车库没车的情况下车的位置已经变成确定的了，概率为2/3。而原来你车库有车的选项却不受到这一事件的影响(依然1/3概率)，所以你当然要选择换车库。这个例子第一个说明的道理是概率是主观的，来自于你头脑中的信息。主持人的举动增加了你对两个车库的信息，而车是不变的，所以你要根据新的信息调整概率空间。
此实例是好的思维方法的力量的典范，如果你没有这个事件空间的角度，恐怕要做无数的试验了。    
条件概率：现实生活中的一般都以条件概率的形式出现，即给定一定的已知条件，我们会得到什么样的概率。对这一大类问题可以引出整个贝叶斯分析理论，将在后续篇章中介绍。

### 2. 随机变量
你投掷筛子，可能得到的结果有6个，每种结果有 1/6 的可能。你把态空间的种种可能性都用数字表达出来，就是随机变量。 这个东西包含所有输出的可能性以及相应的概率，这些可能性(态空间)和概率的对应关系我们称之为分布函数。如果态空间是连续的，我们就得到连续的分布函数形式。
<br>
![](https://raw.githubusercontent.com/21moons/memo/master/res/img/)
<p align="center"><font color=gray size=2>一个二维高斯分布</font></p>
<br>
分布函数：随机变量已经包含了两个随机过程研究的核心武器：态空间和分布函数。分布函数是提取随机过程内有用信息的第一手段。分布函数-是在大量数据中提取信息的入口。
<br>
随机变量的实现：随机变量可以看做一个实验，你在实验之前，结果是不确定的，你所有的是一团可能性。 当你做完实验，却得到一个唯一的结果，只是预先不可知。
<br>
期望： 对一个随机变量，已知其分布函数，可以定义一个期望。这个东西由每个结果的取值和它的可能性共同决定，表达未来结果的加权平均值。 
<br>
实际中我们可以用实验的方法确定这个数字，就是所谓蒙特卡罗方法，不停的投筛子然后做个统计，你所得到的结果的平均就是期望。(平均值和期望的区别就是第一个来自已有的数据的平均，第二是对根据已有的平均对未来的预测)。
关于期望包含着一种投资世界里的基本思维方式，就是对收益的幅值和风险(概率)一起考虑。经常有一些时候一些出现机会极少而收益特别大的可能性决定了期望，如果你的心脏足够强大，就应该充分考虑这些高风险高收益的可能。  
<br>
相关性：  对于两个随机变量，你可以定义一个相关性covariance，描述一个随机变量随另一个而变化的趋势。这个函数特别有用，它是现实生活中我们说两个事物相关性的精确表达。
<br>


$$\sigma(x, y) = \mathrm{E}[(x-\mathrm{E}[x])(y-\mathrm{E}[y])]$$

理解这个算式特别简单，E[x] 是随机变量 x 的期望值(期望值是该变量输出值的平均数), 这个公式就是计算 x 和 y 波动乘积的期望值，当两个变量是此消彼长，则为负，共生共荣则为正，若两个过程不相关，则为 0.
方差：  上述关系当 x=y 我们得到方差，方差就是自己和自己的关联函数，当随机变量比较接近正态分布时候它可以描绘波动性的大小。对于 N 个随机变量，任意两个随机变量可得到一个 covariance，而这样一组 covariance 构成大名鼎鼎的 covariance matrix.
<br>
$$\Sigma = \begin{pmatrix} \mathrm{E}[(\mathrm{X}_1-\mu_1)(\mathrm{X}_1-\mu_1)] & \mathrm{E}[(\mathrm{X}_1-\mu_1)(\mathrm{X}_2-\mu_2)] & \cdots & \mathrm{E}[(\mathrm{X}_1-\mu_1)(\mathrm{X}_n-\mu_n)] \\ \mathrm{E}[(\mathrm{X}_2-\mu_2)(\mathrm{X}_1-\mu_1)] & \mathrm{E}[(\mathrm{X}_2-\mu_2)(\mathrm{X}_2-\mu_2)] & \cdots & \mathrm{E}[(\mathrm{X}_2-\mu_2)(\mathrm{X}_n-\mu_n)] \\ \vdots & \vdots & \vdots & \vdots\\ \mathrm{E}[(\mathrm{X}_n-\mu_n)(\mathrm{X}_1-\mu_1)] & \mathrm{E}[(\mathrm{X}_n-\mu_n)(\mathrm{X}_2-\mu_2)]  & \cdots & \mathrm{E}[(\mathrm{X}_n-\mu_n)(\mathrm{X}_n-\mu_n)] \end{pmatrix}$$

